{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4748798,"sourceType":"datasetVersion","datasetId":2716794},{"sourceId":12428255,"sourceType":"datasetVersion","datasetId":7798736}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Xai Project: Evaluating Explainability\nIn this work we fine-tuned ResNet-18 for a binary task, exploiting the \"Cats and Dogs\" image classification dataset. We then evaluated a set of explainability methods using the Captum library. We will use primarly Grounded SAM model to segment the image and extract the patches of each image.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n# Enable it for DEMO\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nstart_notebook_time = datetime.now() #for time lenght of the whole notebook","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:44:22.616168Z","iopub.execute_input":"2025-07-11T09:44:22.616328Z","iopub.status.idle":"2025-07-11T09:44:22.624155Z","shell.execute_reply.started":"2025-07-11T09:44:22.616313Z","shell.execute_reply":"2025-07-11T09:44:22.623482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install captum\n!pip install autodistill-grounded-sam autodistill-yolov8\n!pip install roboflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:44:22.625572Z","iopub.execute_input":"2025-07-11T09:44:22.626116Z","iopub.status.idle":"2025-07-11T09:44:32.511762Z","shell.execute_reply.started":"2025-07-11T09:44:22.626087Z","shell.execute_reply":"2025-07-11T09:44:32.510803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from autodistill_grounded_sam import GroundedSAM\nfrom autodistill.detection import CaptionOntology\nfrom autodistill.utils import plot\n\nparts = [\"head\", \"body\", \"tail\", \"ear\", \"eye\", \"nose\", \"mouth\", \"paw\"]\n\n\n# Class-specific ontologies\nontology_dog = CaptionOntology({f\"dog {part}\": part for part in parts})\nontology_cat = CaptionOntology({f\"cat {part}\": part for part in parts})\n\n# Pre-initialize both models\nsam_dog = GroundedSAM(ontology=ontology_dog)\nsam_cat = GroundedSAM(ontology=ontology_cat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:44:32.512895Z","iopub.execute_input":"2025-07-11T09:44:32.513144Z","iopub.status.idle":"2025-07-11T09:45:04.232599Z","shell.execute_reply.started":"2025-07-11T09:44:32.513117Z","shell.execute_reply":"2025-07-11T09:45:04.232012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms as T\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset\nimport torch\nimport os\nn_samples_to_explain = 50\n\n\nclass ImageFolderWithPaths(ImageFolder):\n    def __getitem__(self, index):\n        img, label = super().__getitem__(index)\n        path = self.samples[index][0]\n        return img, label, path\n\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Paths\ntrain_dir = '/kaggle/input/cats-and-dogs-image-classification/train'  # adjust if needed\ntest_dir = '/kaggle/input/cats-and-dogs-image-classification/test'  # adjust if needed\n\n# 1. Data transformations\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406],  # ImageNet mean\n                [0.229, 0.224, 0.225])  # ImageNet std\n])\n\n# 2. Load dataset\nif not os.path.exists(\"/kaggle/input/pre-computations/best_model.pth\"):\n    train_dataset = ImageFolder(root=train_dir, transform=transform)\n    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    \ntest_dataset = ImageFolderWithPaths(root=test_dir, transform=transform)\n\ncat_indices = list(range(n_samples_to_explain // 2))\ndog_indices = list(range(len(test_dataset) - n_samples_to_explain // 2, len(test_dataset)))\n\ncat_subset = Subset(test_dataset, cat_indices)\ndog_subset = Subset(test_dataset, dog_indices)\n\nmerged_dataset = ConcatDataset([cat_subset, dog_subset])\nsubset_loader = DataLoader(merged_dataset, batch_size=1, shuffle=False)\n\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.233422Z","iopub.execute_input":"2025-07-11T09:45:04.233998Z","iopub.status.idle":"2025-07-11T09:45:04.456653Z","shell.execute_reply.started":"2025-07-11T09:45:04.233975Z","shell.execute_reply":"2025-07-11T09:45:04.456068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ResNet18 Fine-Tuning","metadata":{}},{"cell_type":"code","source":"from torchvision import datasets, models\nfrom torch import nn, optim\n\nif not os.path.exists(\"/kaggle/input/pre-computations/best_model.pth\"):\n    # Load pre-trained ResNet-18\n    model = models.resnet18(pretrained=True)\n    \n    # Replace the final layer for binary classification\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: cats and dogs\n    model = model.to(device)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Training loop\n    epochs = 20\n    best_accuracy = 0.0\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n    \n        for images, labels in train_dataloader:\n            images, labels = images.to(device), labels.to(device)\n    \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n    \n            running_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += torch.sum(preds == labels).item()\n    \n        epoch_loss = running_loss / len(train_dataloader)\n        train_accuracy = correct / len(train_dataset)\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n    \n        # Evaluation loop\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels, _ in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        \n        test_accuracy = correct / total\n        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n        if test_accuracy > best_accuracy:\n            best_accuracy = test_accuracy\n            torch.save(model.state_dict(), \"best_model.pth\")\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.457347Z","iopub.execute_input":"2025-07-11T09:45:04.457552Z","iopub.status.idle":"2025-07-11T09:45:04.465514Z","shell.execute_reply.started":"2025-07-11T09:45:04.457535Z","shell.execute_reply":"2025-07-11T09:45:04.464804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Medoid Computations\nFor each class we compute the medoid, which is the sample that best represent all the others belonging to the same class.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import pairwise_distances\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom torchvision.datasets import CIFAR10\n\n\nif not os.path.exists(\"/kaggle/input/pre-computations/medoid_class_0.png\") and not os.path.exists(\"/kaggle/input/pre-computations/medoid_class_1.png\"):\n    \n    \n    # Device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load pretrained model (feature extractor)\n    model_medoid = models.resnet50(pretrained=True)\n    model_medoid.fc = torch.nn.Identity()  # rimuove l'ultimo strato per ottenere feature\n    model_medoid = model_medoid.to(device).eval()\n    \n    # Pick one image\n    model_medoid.eval()\n    # Transformation\n    transform = T.Compose([\n        T.Resize(224),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406],  # standard ImageNet normalization\n                    std=[0.229, 0.224, 0.225])\n    ])\n    \n    class_names = test_dataset.classes\n    \n    if medoid_class==\"cat\":\n        target_class = 0\n    else:\n        target_class = 1\n    \n    idxs = [i for i, (_, label) in enumerate(test_dataset) if label == target_class]\n    subset = Subset(test_dataset, idxs)\n    loader = DataLoader(subset, batch_size=32, shuffle=False)\n    \n    # Extract features for each image\n    features = []\n    images = []\n    \n    with torch.no_grad():\n        for x, _ in loader:\n            x = x.to(device)\n            feats = model_medoid(x).cpu().numpy()\n            features.append(feats)\n            images.append(x.cpu())\n    \n    features = np.concatenate(features, axis=0)\n    images = torch.cat(images, dim=0)\n    \n    # Pair-wise matrix\n    dists = pairwise_distances(features, metric='euclidean')\n    \n    # Calculate medoid (minima distanza media)\n    mean_dists = dists.mean(axis=1)\n    medoid_idx = np.argmin(mean_dists)\n    \n    # Visualise Medoid\n    unnormalize = T.Normalize(mean=[-m/s for m, s in zip([0.485, 0.456, 0.406],\n                                                         [0.229, 0.224, 0.225])],\n                              std=[1/s for s in [0.229, 0.224, 0.225]])\n    \n    medoid_img = unnormalize(images[medoid_idx]).permute(1, 2, 0).numpy()\n    medoid_img = np.clip(medoid_img, 0, 1)\n    \n    plt.imshow(medoid_img)\n    plt.axis(\"off\")\n    plt.title(f\"{class_names[target_class]} medoid\")\n    plt.show()\n    \n    plt.imsave(f\"medoid_class_{target_class}.png\", medoid_img)\n\nelse:\n    for img_path in [\"/kaggle/input/pre-computations/medoid_class_0.png\", \"/kaggle/input/pre-computations/medoid_class_1.png\"]:\n        image = cv2.imread(img_path)\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        plt.imshow(image_rgb)\n        title_str = \"cat medoid\" if \"0\" in img_path else \"dog medoid\"\n        plt.title(f\"{title_str}\")\n        plt.axis(\"off\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.466272Z","iopub.execute_input":"2025-07-11T09:45:04.466504Z","iopub.status.idle":"2025-07-11T09:45:04.800336Z","shell.execute_reply.started":"2025-07-11T09:45:04.466482Z","shell.execute_reply":"2025-07-11T09:45:04.799662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gradient-based methods\nWe adopted Captum as a library to explain the model.","metadata":{}},{"cell_type":"code","source":"from captum.attr import IntegratedGradients, Saliency, Occlusion, LayerGradCam, InputXGradient, GuidedBackprop\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport time\n\n#Traform in numpy image \ndef unnormalize(img_tensor, mean, std):\n    if isinstance(mean, torch.Tensor):\n        mean = mean.tolist()\n    if isinstance(std, torch.Tensor):\n        std = std.tolist()\n\n    img = img_tensor.clone().detach().cpu().numpy()\n    for c in range(3):\n        img[c] = img[c] * std[c] + mean[c]\n    return np.clip(img.transpose(1, 2, 0), 0, 1)\n\ndef compute_gradient_methods(model, image, predicted_class, show=False, use_absolute=False):\n    attributions_gray = {}\n    timings = {}\n\n    input_tensor = image.to(device)\n    mean = [0.485, 0.456, 0.406]\n    std  = [0.229, 0.224, 0.225]\n    \n    orig_np = unnormalize(image.squeeze(0), mean, std)\n\n    # GradCAM layer (last ResNet block)\n    gradcam_layer = model.layer4[-1]\n\n    # --- GuidedBackprop (per Guided GradCAM) ---\n    start_guided = time.time()\n    guided_bp = GuidedBackprop(model).attribute(input_tensor, target=predicted_class)\n\n    # --- GradCAM ---\n    start = time.time()\n    layer_gc = LayerGradCam(model, gradcam_layer).attribute(input_tensor, target=predicted_class)\n    timings[\"GradCAM\"] = time.time() - start\n\n    # --- Upsample + Guided GradCAM ---\n    layer_gc_upsampled = torch.nn.functional.interpolate(\n        layer_gc, size=(224, 224), mode='bilinear', align_corners=False\n    )\n    guided_gradcam = guided_bp * layer_gc_upsampled\n    timings[\"Guided GradCAM\"] = time.time() - start_guided\n\n    # --- Altri metodi ---\n    methods = {}\n\n    start = time.time()\n    methods[\"Integrated Gradients\"] = IntegratedGradients(model).attribute(input_tensor, target=predicted_class, n_steps=200)\n    timings[\"Integrated Gradients\"] = time.time() - start\n\n    start = time.time()\n    methods[\"Saliency\"] = Saliency(model).attribute(input_tensor, target=predicted_class)\n    timings[\"Saliency\"] = time.time() - start\n\n    start = time.time()\n    methods[\"Occlusion\"] = Occlusion(model).attribute(\n        input_tensor, strides=(1, 3, 3), target=predicted_class,\n        sliding_window_shapes=(1, 15, 15), baselines=0)\n    timings[\"Occlusion\"] = time.time() - start\n\n    start = time.time()\n    methods[\"Input × Gradient\"] = InputXGradient(model).attribute(input_tensor, target=predicted_class)\n    timings[\"Input × Gradient\"] = time.time() - start\n\n    methods[\"GradCAM\"] = layer_gc\n    methods[\"Guided GradCAM\"] = guided_gradcam\n\n    # --- Visualizzation ---\n    if show:\n        fig, axs = plt.subplots(1, len(methods) + 1, figsize=(15, 5))\n        axs[0].imshow(orig_np)\n        axs[0].set_title(\"Original\")\n        axs[0].axis(\"off\")\n\n    for i, (name, attr) in enumerate(methods.items(), start=1):\n        attr_np = attr.squeeze().detach().cpu().numpy()\n        \n        if attr_np.ndim == 3:\n            attr_gray = np.sum(attr_np, axis=0) if not use_absolute else np.sum(np.abs(attr_np), axis=0)\n        else:\n            attr_gray = attr_np if not use_absolute else np.abs(attr_np)\n\n        attr_vis = (attr_gray - attr_gray.min()) / (attr_gray.max() - attr_gray.min() + 1e-8)\n        attributions_gray[name] = attr_gray\n\n        if show:\n            axs[i].imshow(orig_np, alpha=0.3)\n            axs[i].imshow(attr_gray, cmap=\"seismic\", alpha=0.7, vmin=-np.max(np.abs(attr_gray)), vmax=np.max(np.abs(attr_gray)))\n            axs[i].set_title(name)\n            axs[i].axis(\"off\")\n\n    if show:\n        plt.tight_layout()\n        plt.show()\n\n    return attributions_gray, timings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.801364Z","iopub.execute_input":"2025-07-11T09:45:04.801605Z","iopub.status.idle":"2025-07-11T09:45:04.869898Z","shell.execute_reply.started":"2025-07-11T09:45:04.801586Z","shell.execute_reply":"2025-07-11T09:45:04.869121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIME computation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom skimage.segmentation import mark_boundaries\nfrom captum._utils.models.linear_model import SkLearnLinearRegression\nfrom captum.attr import Lime\nfrom captum.attr import visualization as viz\nimport torch\nimport matplotlib.pyplot as plt\n\ndef compute_LIME(model, image_to_explain, class_predicted, feature_masks, show=False, use_absolute=False): \n\n    unnormalize = T.Normalize(mean=[-m/s for m, s in zip([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])],std=[1/s for s in [0.229, 0.224, 0.225]])\n    orig_np = unnormalize(image_to_explain[0]).permute(1, 2, 0).cpu().numpy()\n    orig_np = np.clip(orig_np, 0, 1)\n    \n    # 1. Construct feature_masks from Grounded-SAM\n    H, W = feature_masks[0].shape\n    feature_mask = np.zeros((H, W), dtype=np.int32)\n    \n    for idx, mask in enumerate(feature_masks):\n        feature_mask[mask] = idx + 1  # background resta 0\n    \n    # Resize feature_mask a 224x224 to matchare orig_np\n    feature_mask_resized = cv2.resize(feature_mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n    if show:\n        # 2. Visualizza masks of Grounded-SAM\n        plt.figure(figsize=(6, 6))\n        plt.imshow(mark_boundaries(orig_np, feature_mask_resized))\n        plt.title(\"LIME SAM segmentation\")\n        plt.axis(\"off\")\n        plt.show()\n        \n        print(\"Feature IDs:\", np.unique(feature_mask).tolist())\n        print(\"Feature mask shape:\", feature_mask.shape)\n        \n    # 3.LIME\n    lr_lime = Lime(model, interpretable_model=SkLearnLinearRegression())\n    \n    feature_mask_tensor = torch.tensor(feature_mask_resized).unsqueeze(0).to(device)\n\n    start = time.time()\n    lime_attr = lr_lime.attribute(\n        image_to_explain,\n        target=class_predicted,\n        feature_mask=feature_mask_tensor\n    ).squeeze(0)\n    duration = time.time() - start\n\n    if show:\n        # 4. Visualize\n        print('Attribution range:', lime_attr.min().item(), 'to', lime_attr.max().item())\n        \n        _ = viz.visualize_image_attr_multiple(\n            lime_attr.permute(1, 2, 0).detach().cpu().numpy(),  # shape [H,W,C]\n            orig_np,\n            [\"original_image\", \"blended_heat_map\", \"blended_heat_map\", \"masked_image\"],\n            [\"all\", \"positive\", \"negative\", \"positive\"],\n            titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"])\n    \n\n    lime_attr_np = lime_attr.detach().cpu().numpy()\n    \n    if lime_attr_np.ndim == 3:\n        if use_absolute:\n            attr_gray = np.sum(np.abs(lime_attr_np), axis=0)\n        else:\n            attr_gray = np.sum(lime_attr_np, axis=0)\n    else:\n        attr_gray = np.abs(lime_attr_np) if use_absolute else lime_attr_np\n    \n    if attr_gray.shape != (224, 224):\n        attr_gray = cv2.resize(attr_gray, (224, 224), interpolation=cv2.INTER_LINEAR)\n    \n    return attr_gray, duration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.872207Z","iopub.execute_input":"2025-07-11T09:45:04.872455Z","iopub.status.idle":"2025-07-11T09:45:04.917506Z","shell.execute_reply.started":"2025-07-11T09:45:04.872436Z","shell.execute_reply":"2025-07-11T09:45:04.916958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SHAP computation","metadata":{}},{"cell_type":"code","source":"from captum.attr import KernelShap\nfrom skimage.segmentation import mark_boundaries\nfrom captum._utils.models.linear_model import SkLearnLinearRegression\nfrom captum.attr import visualization as viz\n\n\ndef compute_SHAP(model, image_to_explain, class_predicted, feature_masks, show=False, use_absolute=False):\n    \n    orig_np = unnormalize(image_to_explain[0], mean, std)\n\n    # Costruct feature_masks Grounded-SAM\n    H, W = feature_masks[0].shape\n    feature_mask = np.zeros((H, W), dtype=np.int32)\n    \n    for idx, mask in enumerate(feature_masks):\n        feature_mask[mask] = idx + 1  # background resta 0\n    \n    # Resize la feature_mask a 224x224 to matchare orig_np\n    feature_mask_resized = cv2.resize(feature_mask, (224, 224), interpolation=cv2.INTER_NEAREST)\n\n    if show:\n        # Visualize Grounded-SAM\n        plt.figure(figsize=(6, 6))\n        plt.imshow(mark_boundaries(orig_np, feature_mask_resized))\n        plt.title(\"Kernel SHAP SAM segmentation\")\n        plt.axis(\"off\")\n        plt.show()\n        \n    # KernelSHAP \n    ks_model = KernelShap(model)\n    \n    start = time.time()\n    ks_attr = ks_model.attribute(\n        image_to_explain,\n        target=class_predicted,\n        n_samples=100,\n        feature_mask=torch.tensor(feature_mask_resized).unsqueeze(0).to(device)\n    ).squeeze(0)\n    duration = time.time() - start\n\n    if show:\n        attr_np = ks_attr.detach().cpu().numpy()\n        min_val = attr_np.min()\n        max_val = attr_np.max()\n        print(f'Attribution range: {min_val:.6f} to {max_val:.6f}')\n        \n        # Check if the attribution is flat\n        if np.abs(max_val - min_val) < 1e-6:\n            print(\"Attribution map is flat (no variation). Skipping visualization.\")\n        else:\n            _ = viz.visualize_image_attr_multiple(\n                ks_attr.permute(1, 2, 0).detach().cpu().numpy(),  # shape: [H, W, C]\n                orig_np,\n                [\"original_image\", \"blended_heat_map\", \"blended_heat_map\", \"masked_image\"],\n                [\"all\", \"positive\", \"negative\", \"positive\"],\n                titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"]\n            )\n\n    \n    ks_attr_np = ks_attr.detach().cpu().numpy()\n    \n    if ks_attr_np.ndim == 3:\n        attr_gray = np.sum(ks_attr_np, axis=0) if not use_absolute else np.sum(np.abs(ks_attr_np), axis=0)\n    else:\n        attr_gray = ks_attr_np if not use_absolute else np.abs(ks_attr_np)\n\n        \n    if attr_gray.shape != (224, 224):\n        attr_gray = cv2.resize(attr_gray, (224, 224), interpolation=cv2.INTER_LINEAR)\n    return attr_gray, duration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.918275Z","iopub.execute_input":"2025-07-11T09:45:04.918884Z","iopub.status.idle":"2025-07-11T09:45:04.928867Z","shell.execute_reply.started":"2025-07-11T09:45:04.918852Z","shell.execute_reply":"2025-07-11T09:45:04.928228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Filtered Masks function\n\nThis function selects just the masks that are necessary to maintain the semantics of the image. All the masks are complementary: they do not overlap.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors as mcolors\nfrom collections import defaultdict\n\n\n\ndef show_masks_on_image(base_image, masks, labels, title, label_colors=None):\n    H, W = base_image.shape[:2]\n    H, W = base_image.shape[:2]\n    overlay = np.ones((H, W, 3), dtype=np.uint8) * 255  # sfondo bianco RGB\n\n\n    for mask, label in zip(masks, labels):\n        # Resize of masks to match image dimensions \n        resized_mask = cv2.resize(mask.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST).astype(bool)\n        color = label_colors[label]\n        for c in range(3):\n            overlay[:, :, c][resized_mask] = color[c]\n\n    plt.imshow(overlay.astype(np.uint8))\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\ndef show_masks_individually(masks, labels, title_prefix=\"\", id_to_label=None, label_colors=None):\n    H, W = image_rgb.shape[:2]\n\n    for i, (mask, label) in enumerate(zip(masks, labels)):\n        resized = cv2.resize(mask.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST).astype(bool)\n\n        overlay = np.ones((H, W, 3), dtype=np.uint8) * 255\n\n        if isinstance(label, str) or id_to_label is None:\n            label_str = str(label)\n        else:\n            label_str = id_to_label.get(label, str(label))\n\n        color_key = label_str\n        color = label_colors.get(color_key, np.array([0, 0, 0]))  # fallback nero\n\n        # Apply masks\n        for c in range(3):\n            overlay[:, :, c][resized] = color[c]\n\n        plt.imshow(overlay)\n        plt.title(f\"{title_prefix}Mask {i+1} — {label_str}\")\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef calculate_filtered_masks(img_path, results, id_to_label, show=False):\n\n    image_bgr = cv2.imread(\"/kaggle/input/cats-and-dogs-image-classification/test/cats/cat_1.jpg\")\n    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    \n    image_rgb = cv2.resize(image_rgb, (224, 224), interpolation=cv2.INTER_AREA)\n    \n    unique_labels = list(set(results.class_id))\n    label_colors = {label: np.random.randint(0, 255, size=3) for label in unique_labels}\n    \n    #  1. Show original masks\n    original_masks = results.mask         # [N, H, W]\n    original_labels = results.class_id    # [N]\n    #show_masks_on_image(image_rgb, original_masks, original_labels, \"Maschere originali da Grounded-SAM\")\n    \n    #  2. Filtraggio intra-classe \n    masks_by_class = defaultdict(list)\n    for mask, label in zip(original_masks, original_labels):\n        masks_by_class[label].append(mask) #-> \"ear\": [mask_ear1, mask_ear2, mask_ear3, ...],\n    \n    filtered_masks = []\n    filtered_labels = []\n    \n    for label, class_masks in masks_by_class.items():\n        accepted = []\n        total_mask = np.zeros_like(class_masks[0], dtype=bool)\n        class_masks = sorted(class_masks, key=lambda m: m.sum()) #dalla più piccola alla più grande\n        for mask in class_masks:\n            if not np.any(np.logical_and(total_mask, mask)): #Se non si sovrappone con nulla già accettato\n                accepted.append(mask)\n                total_mask |= mask\n        filtered_masks.extend(accepted)\n        filtered_labels.extend([label] * len(accepted))\n    \n    # Show filtered masks intra-classe\n    #show_masks_on_image(image_rgb, filtered_masks, filtered_labels, \"Maschere filtrate intra-classe (più piccole, no overlap nella stessa classe)\", label_colors)\n    \n    #  3. Remove overlap inter-classe \n    final_masks = filtered_masks.copy()\n    final_labels = filtered_labels.copy()\n    \n    n = len(final_masks)\n    \n    kernel = np.ones((3, 3), np.uint8)\n    \n    for i in range(n):\n        for j in range(i + 1, n):\n            if final_labels[i] != final_labels[j]:\n                mask_i = final_masks[i]\n                mask_j = final_masks[j]\n    \n                area_i = mask_i.sum()\n                area_j = mask_j.sum()\n    \n                if area_i < area_j:\n                    smaller_mask = cv2.dilate(mask_i.astype(np.uint8), kernel, iterations=1).astype(bool)\n                    intersection = np.logical_and(smaller_mask, mask_j)\n                    \n                    if intersection.sum() > 0:\n                        final_masks[j] = np.logical_and(mask_j, ~intersection)\n                        final_masks[i]=smaller_mask\n                else:\n                    smaller_mask = cv2.dilate(mask_j.astype(np.uint8), kernel, iterations=1).astype(bool)\n                    intersection = np.logical_and(mask_i, smaller_mask)\n                    if intersection.sum() > 0:\n                        final_masks[i] = np.logical_and(mask_i, ~intersection)\n                        final_masks[j]=smaller_mask\n\n    \n    filtered_final_masks = []\n    filtered_final_labels = []\n    for mask, label in zip(final_masks, final_labels):\n        if mask.sum() > 10:\n            filtered_final_masks.append(mask)\n            filtered_final_labels.append(label)\n\n\n    \"\"\"# Refinement finale con operazione morfologica (MORPH_CLOSE)\n    refined_final_masks = []\n    for mask in filtered_final_masks:\n        closed = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel).astype(bool)\n        refined_final_masks.append(closed)\n    \n    filtered_final_masks = refined_final_masks  # Sovrascrive le maschere finali\"\"\"\n    \n    # Show final masks \n    if show:\n        show_masks_on_image(image_rgb, filtered_final_masks, filtered_final_labels, \"All final masks\", label_colors)\n\n    #id_to_label = base_model.ontology.classes() \n    # Se base_model.ontology.classes() è una lista:\n    if isinstance(id_to_label, list):\n        id_to_label = {i: name for i, name in enumerate(id_to_label)}\n    \n    # 1. Cumulative mask for background\n    H, W = filtered_final_masks[0].shape\n    total_mask = np.zeros((H, W), dtype=bool)\n    \n    for mask in filtered_final_masks:\n        total_mask |= mask \n        \n    total_mask_dilated = cv2.dilate(total_mask.astype(np.uint8), kernel, iterations=1).astype(bool)#fix bg\n    \n    background_mask = ~total_mask\n    \n    # 3. Remove rumore troppo piccolo\n    if background_mask.sum() < 5:\n        print(\"Background troppo piccolo, non aggiunto.\")\n    else:\n        next_id = max(id_to_label.keys()) + 1\n        \n        # Add \"background\" with new ID\n        id_to_label[next_id] = \"background\"\n        \n        filtered_final_labels.append(next_id)\n        filtered_final_masks.append(background_mask)\n    \n    if show:\n        #show_masks_individually(original_masks, original_labels, title_prefix=\"ORIG - \", id_to_label=id_to_label, label_colors=label_colors)\n        #print(\"-\"*100)\n        #show_masks_individually(filtered_masks, filtered_labels, title_prefix=\"FILTERED - \", id_to_label=id_to_label, label_colors=label_colors)\n        print(\"-\"*100)\n        show_masks_individually(filtered_final_masks, filtered_final_labels, title_prefix=\"FINAL - \", id_to_label=id_to_label, label_colors=label_colors)\n\n    print(f'Final masks for this instance are: {len(filtered_final_masks)}')\n    return filtered_final_masks, filtered_final_labels, id_to_label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.929814Z","iopub.execute_input":"2025-07-11T09:45:04.930270Z","iopub.status.idle":"2025-07-11T09:45:04.952210Z","shell.execute_reply.started":"2025-07-11T09:45:04.930251Z","shell.execute_reply":"2025-07-11T09:45:04.951584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Rank masks\nThe following function sorts the attributions assigned to patches (extracted by Grounded SAM) in decremental order.","metadata":{}},{"cell_type":"code","source":"\ndef sort_masks(attributions, filtered_final_masks, filtered_final_labels, id_to_label, show=False):\n    # Calcola e stampa ranking per ogni metodo Captum\n    sorted_masks_by_method = {}\n    \n    for method_name, attr_gray in attributions.items():\n        # Resize \n        if attr_gray.shape != (224, 224):\n            attr_gray = cv2.resize(attr_gray, (224, 224), interpolation=cv2.INTER_LINEAR)\n        mask_scores = []\n        for i, (mask, label) in enumerate(zip(filtered_final_masks, filtered_final_labels)):\n            resized_mask = cv2.resize(mask.astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n            mask_area = resized_mask.sum()\n            if mask_area == 0:\n                score = 0\n            else:\n                score = attr_gray[resized_mask].sum() / mask_area\n            mask_scores.append((i, score, label))\n        sorted_masks = sorted(mask_scores, key=lambda x: x[1], reverse=True)\n        sorted_masks_by_method[method_name] = sorted_masks # Used also later for insertion curve\n\n        if show:\n            print(f\"\\n Top masks by mean score for: {method_name}\")\n            for rank, (i, score, label) in enumerate(sorted_masks):                \n                label_name = id_to_label[label]\n                print(f\"Rank {rank+1}: Mask {i} → Label: '{label_name}' | Score = {score:.4f}\")\n    return sorted_masks_by_method","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.953070Z","iopub.execute_input":"2025-07-11T09:45:04.953275Z","iopub.status.idle":"2025-07-11T09:45:04.971546Z","shell.execute_reply.started":"2025-07-11T09:45:04.953258Z","shell.execute_reply":"2025-07-11T09:45:04.970964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature removal visualization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef feature_removal_visualization(image_to_explain, sorted_masks_by_method, medoid_class, filtered_final_masks, id_to_label):\n    \n    for method_name, sorted_masks in sorted_masks_by_method.items():\n        print()\n        print(\"-\"*40)\n        print(f\"\\nVisualizing step-by-step for the method: {method_name}\")\n        mean = [0.485, 0.456, 0.406]\n        std  = [0.229, 0.224, 0.225]\n        orig_np = unnormalize(image_to_explain[0], mean, std)\n        \n        modified_image = orig_np.copy()\n    \n        # Load the correct medoid\n        if medoid_class == \"cat\":\n            bgr = cv2.imread(\"/kaggle/input/pre-computations/medoid_class_0.png\")\n        else:\n            bgr = cv2.imread(\"/kaggle/input/pre-computations/medoid_class_1.png\")\n    \n        medoid = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n        assert medoid.shape == modified_image.shape, \"Shape mismatch between images!\"\n    \n        # 5 masks\n        for i, (index, score, label) in enumerate(sorted_masks):\n            mask = cv2.resize(filtered_final_masks[index].astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n    \n            # Applly medoid\n            for c in range(3):  # R, G, B channels\n                modified_image[mask, c] = medoid[mask, c]\n            print(f\"Label {label} is {id_to_label[label]}, id_to_label has len: {len(id_to_label)}\")\n            label_name = id_to_label[label]\n    \n            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n            axs[0].imshow(modified_image)\n            axs[0].set_title(f\"{method_name} - Step {i+1}: Replaced '{label_name}'\")\n            axs[0].axis('off')\n    \n            axs[1].imshow(mask, cmap='gray')\n            axs[1].set_title(f\"Mask of '{label_name}'\")\n            axs[1].axis('off')\n    \n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.972481Z","iopub.execute_input":"2025-07-11T09:45:04.972749Z","iopub.status.idle":"2025-07-11T09:45:04.991138Z","shell.execute_reply.started":"2025-07-11T09:45:04.972722Z","shell.execute_reply":"2025-07-11T09:45:04.990294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Effective Compactness","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms as T\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Parameters\nuse_softmax = True     \nthreshold = 0.5  #Used if softmax is True\n\n\ntransform = T.Compose([\n    T.Resize(224),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225])\n])\n\ndef compute_EC(original_label, original_prob, sorted_masks_by_method, image_to_explain, model, medoid_class, original_class, show=False):\n\n    if show:\n        print(f\" Class originally assigned: {original_label}\")\n        print(f\" Initial probability: {original_prob}\\n\")\n\n    \n    orig_np = unnormalize(image_to_explain[0], mean, std)\n\n    method_to_ec = {}\n    \n    for method_name, sorted_masks in sorted_masks_by_method.items():\n        if show:\n            print(f\"\\n Method: {method_name}\")\n\n        orig_copy = orig_np.copy()\n        probs = [original_prob]\n        misclassification = False\n        ec = len(sorted_masks)\n    \n        for i, (index, score, label) in enumerate(sorted_masks):\n            mask = cv2.resize(filtered_final_masks[index].astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n\n            if medoid_class == \"cat\":\n                bgr = cv2.imread(\"/kaggle/input/pre-computations/medoid_class_0.png\")\n            else:\n                bgr = cv2.imread(\"/kaggle/input/pre-computations/medoid_class_1.png\")\n\n            bgr = cv2.resize(bgr, (224, 224))  \n            medoid = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n            \n            # Apply medoid\n            for c in range(3):\n                orig_copy[mask, c] = medoid[mask, c]\n\n            mod_uint8 = (orig_copy * 255).astype(np.uint8)\n            mod_pil = Image.fromarray(mod_uint8)\n            input_mod = transform(mod_pil).unsqueeze(0).to(device)\n        \n            with torch.no_grad():\n                pred_mod = model(input_mod)\n    \n            pred_class = pred_mod.argmax(dim=1).item()\n            pred_name = \"cat\" if pred_class == 0 else \"dog\"\n            prob = torch.softmax(pred_mod, dim=1)[0, original_class].item()\n            probs.append(prob)\n    \n            label_name = id_to_label[label]\n            if show:\n                print(f\"{i+1}) Removal of '{label_name}' → Class: {pred_name} | Probability: {prob}\")\n    \n            if use_softmax:\n                if prob < threshold and not misclassification:\n                    misclassification = True\n                    ec = i + 1\n                    if show:\n                        print(f\" [EC] Probability under {threshold:.2f} after {ec} masks → EC = {ec}\\n\")\n                    break\n            else:\n                if pred_class != original_class:\n                    ec = i + 1\n                    if show:\n                        print(f\" [EC] Class changed after {ec} masks → EC = {ec}\\n\")\n                    break\n        if show: \n            print(f\" Effective Compactness for {method_name}: {ec}\\n\")\n        \n        method_to_ec[method_name] = ec\n    \n    return method_to_ec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:04.992192Z","iopub.execute_input":"2025-07-11T09:45:04.992975Z","iopub.status.idle":"2025-07-11T09:45:05.010619Z","shell.execute_reply.started":"2025-07-11T09:45:04.992954Z","shell.execute_reply":"2025-07-11T09:45:05.009801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RQI","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport torch\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import auc\nfrom copy import deepcopy\n\n#  Number of permutations \nn_permutations = 100\n\n# compute deletion curve \ndef compute_deletion_curve(mask_order, base_image, model, original_class, medoid, device):\n    modified = deepcopy(base_image)\n    probs = []\n    \n    for raw_index, _, _ in mask_order:\n        index = int(raw_index)\n        mask = cv2.resize(filtered_final_masks[index].astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n        for c in range(3):\n            modified[mask, c] = medoid[mask, c]\n        mod_uint8 = (modified * 255).astype(np.uint8)\n        mod_pil = Image.fromarray(mod_uint8)\n        input_mod = transform(mod_pil).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            pred = model(input_mod)\n        prob = torch.softmax(pred, dim=1)[0, original_class].item()\n        probs.append(prob)\n\n    return np.array(probs)\n\ndef compute_RQI(image_to_explain, model, original_prob, original_class, medoid_class, sorted_masks_by_method, show=False):\n    \n    orig_np = unnormalize(image_to_explain[0], mean, std)\n    if medoid_class == \"cat\":\n        bgr = cv2.imread(\"/kaggle/input/pre-computations/medoid_class_0.png\")\n    else:\n        bgr = cv2.imread(\"/kaggle/input/pre-computations/medoid_class_1.png\")\n\n    bgr = cv2.resize(bgr, (224, 224))  # resize before converting to RGB\n    medoid = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n    method_to_rqi = {}\n    # Computations applied for each method \n    for method_name, sorted_masks in sorted_masks_by_method.items():\n        if show:\n            print(f\"\\n Metodo: {method_name}\")\n    \n        # 1. Explained curve\n        explained_curve = compute_deletion_curve(sorted_masks, orig_np, model, original_class, medoid, device)\n        explained_curve = np.concatenate(([original_prob], explained_curve))\n        x_norm = np.linspace(0, 1, len(explained_curve))\n        audc_explained = auc(x_norm, explained_curve)\n    \n        # 2. Random curve\n        successes = 0\n        failures = 0\n        random_curves = []\n    \n        for _ in range(n_permutations):\n            random_order = [tuple((int(i), s, l)) for i, s, l in np.random.permutation(sorted_masks)]\n            random_curve = compute_deletion_curve(random_order, orig_np, model, original_class, medoid, device)\n            random_curve = np.concatenate(([original_prob], random_curve))\n            \n            random_curves.append(random_curve)\n    \n            for pe, pr in zip(explained_curve, random_curve):\n                if pe <= pr:\n                    successes += 1\n                else:\n                    failures += 1\n    \n        # 3. RQI\n        rqi_score = successes / (successes + failures)\n\n        method_to_rqi[method_name] = rqi_score\n\n        if show:\n            print(f\" RQI = {rqi_score:.4f}, computed on {n_permutations} permutations\")\n            print(f\" AUDC = {audc_explained:.4f}\")\n    \n        \n            # 4. Plot\n            plt.figure(figsize=(8, 5))\n            for curve in random_curves:\n                plt.plot(range(len(curve)), curve, color='gray', alpha=0.4, linewidth=1)\n            plt.plot(range(len(explained_curve)), explained_curve, color='teal', marker='o', linewidth=2, label='Explainer')\n        \n            plt.title(f\"Deletion Curve - {method_name}\")\n            plt.xlabel(\"Numero maschere rimosse\")\n            plt.ylabel(\"Probabilità classe originale\")\n            plt.legend()\n            plt.grid(True)\n            plt.tight_layout()\n            plt.show()\n\n    return method_to_rqi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:05.011583Z","iopub.execute_input":"2025-07-11T09:45:05.012097Z","iopub.status.idle":"2025-07-11T09:45:05.029458Z","shell.execute_reply.started":"2025-07-11T09:45:05.012071Z","shell.execute_reply":"2025-07-11T09:45:05.028827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Instability","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms as T\n\nmean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\n\ndef compute_instability(image_to_explain, model, original_prob, original_class, filtered_final_masks, number_of_experiments=10, show=False):\n        \n    attributions = {}\n    \n    for i in range(number_of_experiments):\n        if show:\n            print(f\" Run {i+1}/{number_of_experiments}\")\n    \n        # Gradient-based methods (es. IG, Saliency...)\n        attr_captum, _ = compute_gradient_methods(model, image_to_explain, original_class)\n    \n        # Perturbation-based methods (es. LIME, SHAP...)\n        attr_lime, _ = compute_LIME(model, image_to_explain, original_class, filtered_final_masks)\n        attr_shap, _ = compute_SHAP(model, image_to_explain, original_class, filtered_final_masks)\n    \n        for method_name, attr in attr_captum.items():\n            if method_name not in attributions:\n                attributions[method_name] = []\n            attributions[method_name].append(attr)\n    \n        for name, attr in {\"LIME (SAM)\": attr_lime, \"Kernel SHAP (SAM)\": attr_shap}.items():\n            if name not in attributions:\n                attributions[name] = []\n            attributions[name].append(attr)\n\n    method_to_instability = {}\n    \n    #  Sufficiency Score computation for each method \n    if show:\n        print(\"\\n Sufficiency Score for each method:\")\n        \n    for method_name, attr_list in attributions.items():\n        if show: \n            print(f\"\\n Method: {method_name}\")\n        \n        score_matrix = []\n    \n        for attr_gray in attr_list:\n            if attr_gray.shape != (224, 224):\n                attr_gray = cv2.resize(attr_gray, (224, 224), interpolation=cv2.INTER_LINEAR)\n    \n            scores = []\n            for mask in filtered_final_masks:\n                resized_mask = cv2.resize(mask.astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n                mask_area = resized_mask.sum()\n                if mask_area == 0:\n                    scores.append(0)\n                else:\n                    scores.append(attr_gray[resized_mask].sum() / mask_area)\n    \n            score_matrix.append(scores)\n    \n        A_x = np.array(score_matrix)  \n        A_norm = A_x / np.max(A_x)    \n    \n        M = np.mean(np.abs(A_norm), axis=0)\n        Sigma = np.std(A_norm, axis=0)\n\n        instability_score_x = np.mean(M * Sigma)\n        \n        stability_score_x = 1 - instability_score_x\n        \n        method_to_instability[method_name] = instability_score_x\n        \n        if show:\n            print(f\" Sufficiency Score = {stability_score_x:.4f}\")\n        \n    return method_to_instability\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:05.030268Z","iopub.execute_input":"2025-07-11T09:45:05.030480Z","iopub.status.idle":"2025-07-11T09:45:05.047084Z","shell.execute_reply.started":"2025-07-11T09:45:05.030463Z","shell.execute_reply":"2025-07-11T09:45:05.046377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Valley Score  \nMeasures how well the model’s confidence follows a valley-shaped trajectory when patches are inserted in increasing order of importance.  \nHigher values indicate that less important features reduce confidence before more important ones restore it, reflecting a faithful contrastive ranking.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.metrics import auc\nfrom copy import deepcopy\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms as T\n\n# Parametri \nthreshold_insertion = 0.8\nbackground_color = 1.0 \n\ndef compute_score(values):\n    if len(values) < 3:\n        return 0.0\n\n    min_index = np.argmin(values)\n    if min_index == 0 or min_index == len(values) - 1:\n        return 0.0\n\n    left = values[:min_index+1]\n    right = values[min_index:]\n\n    left_diffs = np.diff(left)\n    right_diffs = np.diff(right)\n\n    left_score = np.sum(left_diffs <= 0) / len(left_diffs) if len(left_diffs) > 0 else 0\n    right_score = np.sum(right_diffs >= 0) / len(right_diffs) if len(right_diffs) > 0 else 0\n\n    return (left_score + right_score) / 2\n\ndef compute_valley_score(image_to_explain, attributions, sorted_masks_by_method, model, filtered_final_masks, original_class, show = False):\n    \n    # insertion_counts = {}\n    orig_np = unnormalize(image_to_explain[0], mean, std)\n    valley_scores = {}\n    \n    for method in attributions.keys():\n        # insertion_counts[method] = 1    \n    \n        attr_gray = attributions[method]\n        sorted_masks = list(reversed(sorted_masks_by_method[method]))  # lista di tuple (index, score, label)\n    \n        transform = T.Compose([\n            T.Resize(224),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n        ])\n        \n        insertion_image = np.ones_like(orig_np) * background_color\n        probs = []\n    \n        # Prediction on the white image\n        mod_uint8 = (insertion_image * 255).astype(np.uint8)\n        mod_pil = Image.fromarray(mod_uint8)\n        input_mod = transform(mod_pil).unsqueeze(0).to(device)\n    \n        with torch.no_grad():\n            pred_mod = model(input_mod)\n        prob_white = torch.softmax(pred_mod, dim=1)[0, original_class].item()\n        probs.append(prob_white)\n    \n        # Loop on the ordered masks and built progressively the image adding the original image patches\n        for index, _, _ in sorted_masks:\n            mask = cv2.resize(filtered_final_masks[index].astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n            for c in range(3):\n                insertion_image[mask, c] = orig_np[mask, c]  # inserisci pixel originali\n    \n            mod_uint8 = (insertion_image * 255).astype(np.uint8)\n            mod_pil = Image.fromarray(mod_uint8)\n            input_mod = transform(mod_pil).unsqueeze(0).to(device)\n    \n            with torch.no_grad():\n                pred_mod = model(input_mod)\n            prob = torch.softmax(pred_mod, dim=1)[0, original_class].item()\n            \"\"\"\n            if prob < threshold_insertion:\n                insertion_counts[method] += 1\n            \"\"\"\n            probs.append(prob)\n\n        score = compute_score(probs)\n        \n        valley_scores[method] = score\n        if show: \n            \n            print(f\"Valley Score ({method}): {score:.4f}\")\n            \n        # Plot the curve\n            steps = list(range(len(probs)))\n            plt.plot(steps, probs, marker='o', color='seagreen', linewidth=2)\n            plt.fill_between(steps, probs, color='lightgreen', alpha=0.4)\n            plt.xticks(steps)\n            plt.xlabel(\"Number of added masks\")\n            plt.ylabel(\"Original class probability\")\n            plt.title(f\"Insertion Curve ({method})\")\n            plt.legend()\n            plt.grid(True)\n            plt.tight_layout()\n            plt.show()\n            \n        # Compute AUC\n        x_norm = np.linspace(0, 1, len(probs))\n        auc_insertion = auc(x_norm, probs)\n        if show:\n            print(f\"Insertion AUC = {auc_insertion:.4f}  ({method})\")\n            print()\n    \n    \"\"\"\n    print(f\"\\n Number of insertions necessary to reach probability threshold {threshold_insertion}\")\n    for method in insertion_counts.keys():\n        print(f'{method}: {insertion_counts[method]}')\n    \"\"\"\n    return valley_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:05.047874Z","iopub.execute_input":"2025-07-11T09:45:05.048111Z","iopub.status.idle":"2025-07-11T09:45:05.070718Z","shell.execute_reply.started":"2025-07-11T09:45:05.048085Z","shell.execute_reply":"2025-07-11T09:45:05.069906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sufficiency Score\nDifference between the model's confidence on the full image and the same image reconstructed using only the patches removed during Effective Compactness.\nLower values indicate that EC-selected patches are sufficient to explain the model's prediction.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport cv2\n\ndef compute_sufficiency_ec(attributions, sorted_masks_by_method, ec_values, model, filtered_final_masks, image_to_explain, original_class, show=False):\n    \"\"\"\n    Calcola una metrica di sufficienza inversa a partire dalle maschere usate nella EC,\n    valutando quanto quelle maschere, quando ricombinate su sfondo bianco, bastano a spiegare la predizione originale.\n    Mostra anche l'immagine originale e la ricostruzione su sfondo bianco.\n    \"\"\"\n    from torchvision import transforms as T\n    from PIL import Image\n\n    sufficiency_scores = {}\n    orig_np = unnormalize(image_to_explain[0], mean, std)\n\n    for method in attributions.keys():\n        k = ec_values[method]\n        if k == 0:\n            sufficiency_scores[method] = 1.0\n            continue\n\n        ec_indices = [index for index, _, _ in sorted_masks_by_method[method][:k]]\n        reconstructed = np.ones_like(orig_np) * 1.0\n\n        for idx in ec_indices:\n            mask = cv2.resize(filtered_final_masks[idx].astype(np.uint8), (224, 224), interpolation=cv2.INTER_NEAREST).astype(bool)\n            for c in range(3):\n                reconstructed[mask, c] = orig_np[mask, c]\n\n        transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n        ])\n\n        input_full = transform(orig_np).unsqueeze(0).to(device)\n        with torch.no_grad():\n            pred_full = model(input_full)\n        prob_full = torch.softmax(pred_full, dim=1)[0, original_class].item()\n\n        mod_uint8 = (reconstructed * 255).astype(np.uint8)\n        mod_pil = Image.fromarray(mod_uint8)\n        input_reconstructed = transform(mod_pil).unsqueeze(0).to(device)\n        with torch.no_grad():\n            pred_new = model(input_reconstructed)\n        prob_new = torch.softmax(pred_new, dim=1)[0, original_class].item()\n\n        delta = prob_full - prob_new\n        sufficiency_scores[method] = delta\n\n        if show:\n            print(f\"[{method}] P(full) = {prob_full:.4f} | P(EC-mask) = {prob_new:.4f} → Δ = {delta:.4f}\")\n            \n            # Visualize image original e reconstructed\n            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n            axs[0].imshow(orig_np)\n            axs[0].set_title(\"Original Image\")\n            axs[0].axis(\"off\")\n\n            axs[1].imshow(reconstructed)\n            axs[1].set_title(f\"EC patches ({method})\")\n            axs[1].axis(\"off\")\n            plt.suptitle(f\"SufficiencyEC – {method}\")\n            plt.tight_layout()\n            plt.show()\n\n    return sufficiency_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:45:05.071576Z","iopub.execute_input":"2025-07-11T09:45:05.072128Z","iopub.status.idle":"2025-07-11T09:45:05.086954Z","shell.execute_reply.started":"2025-07-11T09:45:05.072098Z","shell.execute_reply":"2025-07-11T09:45:05.086327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main loop\nHere we iterate over the 50 samples and call different functions that execute and evaluate the explainability methods adopted in this work","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import models\nfrom torch import nn\nimport numpy as np\nimport cv2\nfrom PIL import Image \nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\"\"\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMAGE_PATH = \"/kaggle/input/cats-and-dogs-image-classification/test/dogs/dog_123.jpg\" # Image to explain\nmean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\"\"\"\n\n\nmodel = models.resnet18(pretrained=False)\nmodel.fc = nn.Linear(model.fc.in_features, 2)\nmodel.load_state_dict(torch.load(\"/kaggle/input/pre-computations/best_model.pth\"))\nmodel = model.to(device).eval()\n# Inference\nmodel.eval()\n\n#ALL SHOW SETUPS\ncount = 0\nshow_SAM = False\nshow_prediction = False\nshow_feature_removal = False\nshow_masks= False\n\nall_ec_values = {}  # key = method_name, value = list of ECs\nall_rqi_values = {}  # key = method_name, value = list of RQIs\nall_instability_values = {}\nall_duration_values = {}\nall_valley_scores = {}\nall_stability_scores = {}\n\nfor i, (image, label, path) in enumerate(subset_loader):\n    print(f\"\\n Analyzing Sample Number: {i+1}/{len(subset_loader)}\")\n    \n    count+=1\n\n    image = image.to(device)\n    label = label.to(device)\n\n    # Get model prediction\n    with torch.no_grad():\n        original_pred = model(image)\n        \n    predicted_class = original_pred.argmax(dim=1).item()\n    original_prob = torch.softmax(original_pred, dim=1)[0, predicted_class].item()\n    assigned_label = \"cat\" if predicted_class == 0 else \"dog\"\n    \n    # Choose medoid whose class is the opposite w.r.t. model class prediction\n    medoid_class = \"cat\" if \"dog\" in assigned_label else \"dog\"\n\n    # Use the correct segmentation model based on prediction\n    if assigned_label == \"cat\":\n        results = sam_cat.predict(path[0]) # Get the segmentation masks\n        id_to_label = sam_cat.ontology.classes() # Get the mask-id → label dictionary\n    \n        if show_SAM:\n            plot(\n                image=cv2.imread(path[0]),\n                classes=sam_cat.ontology.classes(),\n                detections=results\n            ) \n    else:\n        results = sam_dog.predict(path[0])# Get the segmentation masks\n        id_to_label = sam_dog.ontology.classes() # Get the mask-id → label dictionary\n        \n        if show_SAM:\n            plot(\n                image=cv2.imread(path[0]),\n                classes=sam_dog.ontology.classes(),\n                detections=results\n            ) \n\n    # Filter the masks in order to avoid duplicates and to make them complementary (no overlap)\n    filtered_final_masks, filtered_final_labels, id_to_label = calculate_filtered_masks(path[0], results, id_to_label, show_masks)\n\n    # Compute attributions for gradient-based methods, LIME and SHAP \n    print(\"-\"*90)\n    print(\"Computing Gradient methods\")\n    attributions, method_to_duration = compute_gradient_methods(model, image, predicted_class, use_absolute=False, show=False)\n    print(\"-\"*90)\n    print(\"Computing LIME\")\n    attr_lime, duration_lime = compute_LIME(model, image, predicted_class, filtered_final_masks, use_absolute=False, show=False)\n    print(\"-\"*90)\n    print(\"Computing SHAP\")\n    attr_shap, duration_shap = compute_SHAP(model, image, predicted_class, filtered_final_masks, use_absolute=False, show=False)\n    print(\"-\"*90)\n\n\n    # Store in the main dictionary also lime and shap results\n    attributions[\"LIME (SAM)\"] = attr_lime\n    attributions[\"Kernel SHAP (SAM)\"] = attr_shap\n\n    method_to_duration[\"LIME (G-SAM)\"] = duration_lime\n    method_to_duration[\"Kernel SHAP (G-SAM)\"] = duration_shap\n\n    # Sort the masks based on their attributions density (decremental order)\n    sorted_masks_by_method = sort_masks(attributions, filtered_final_masks, filtered_final_labels, id_to_label, show=False)\n\n    # If true, it prints the step-by-step removal of patches\n    if show_feature_removal:\n        feature_removal_visualization(image, sorted_masks_by_method, medoid_class, filtered_final_masks, id_to_label)\n\n    print(\"-\"*90)\n    print(\"Computing EC\")\n    # Compute Effective Compactness for all the methods, considering the current data sample\n    method_to_ec = compute_EC(assigned_label, original_prob, sorted_masks_by_method, image, model, medoid_class, predicted_class, show = False)\n\n    print(\"-\"*90)\n    print(\"Computing RQI\")\n    # Compute RQI for all the methods, considering the current data sample\n    method_to_rqi = compute_RQI(image, model, original_prob, predicted_class, medoid_class, sorted_masks_by_method, show=False)\n\n    print(\"-\"*90)\n    print(\"Computing Instability\")\n    # Compute Instability for all the methods, considering the current data sample\n    method_to_instability = compute_instability(image, model, original_prob, predicted_class, filtered_final_masks, number_of_experiments=10, show = False)\n\n    print(\"-\"*90)\n    print(\"Computing Valley Score\")\n    method_to_valley_score = compute_valley_score(image, attributions, sorted_masks_by_method, model, filtered_final_masks, predicted_class, show=False)\n\n    print(\"-\"*90)\n    print(\"Computing Sufficiency\")\n    method_to_stability_score= compute_sufficiency_ec(attributions, sorted_masks_by_method, method_to_ec, model, filtered_final_masks, image, predicted_class, show=False)\n\n    \n    # Save the association method → list of ECs \n    for method, ec in method_to_ec.items():\n        if method not in all_ec_values:\n            all_ec_values[method] = []\n        all_ec_values[method].append(ec)\n\n    # Save the association method → list of RQIs\n    for method, rqi in method_to_rqi.items():\n        if method not in all_rqi_values:\n            all_rqi_values[method] = []\n        all_rqi_values[method].append(rqi)\n\n    # Save the association method → list of Stabilities \n    for method, instability in method_to_instability.items():\n        if method not in all_instability_values:\n            all_instability_values[method] = []\n        all_instability_values[method].append(instability)\n\n    # Save the association method → list of Average Execution Times\n    for method, duration in method_to_duration.items():\n        if method not in all_duration_values:\n            all_duration_values[method] = []\n        all_duration_values[method].append(duration)\n    \n    # Save the association method → list of Insertioon Counts\n    for method, ms in method_to_valley_score.items():\n        if method not in all_valley_scores:\n            all_valley_scores[method] = []\n        all_valley_scores[method].append(ms)\n\n      # Save the association method → list of Insertioon Counts\n    for method, score in method_to_stability_score.items():\n        if method not in all_stability_scores:\n            all_stability_scores[method] = []\n        all_stability_scores[method].append(score)\n    \n\n    if show_prediction:\n        \n        print(f\"Predicted class: {assigned_label}\")\n        print(f\"Opposite class (medoid): {medoid_class}\")\n\n        img_to_show = image[0].cpu().clone()  # (3, 224, 224)\n        \n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        img_to_show = img_to_show * std + mean \n        \n        img_to_show = img_to_show.permute(1, 2, 0).numpy()\n        img_to_show = np.clip(img_to_show, 0, 1)\n        \n        # Visualizza\n        plt.imshow(img_to_show)\n        plt.title(f\"Predicted class: {assigned_label}\")\n        plt.axis(\"off\")\n        plt.show()\n\nprint(\"-\"*90)\nprint(\"End of samples to expalain\")\nprint()\n\nprint(\"\\n Effective Compactness (Mean ± Std):\")\nfor method, ec_list in all_ec_values.items():\n    ec_array = np.array(ec_list)\n    mean_ec = ec_array.mean()\n    std_ec = ec_array.std()\n    print(f\"- {method}: {mean_ec:.2f} ± {std_ec:.2f}\")\n\nprint(\"\\n Rank Quality Index (Mean ± Std):\")\nfor method, rqi_list in all_rqi_values.items():\n    rqi_array = np.array(rqi_list)\n    mean_rqi = rqi_array.mean()\n    std_rqi = rqi_array.std()\n    print(f\"- {method}: {mean_rqi:.2f} ± {std_rqi:.2f}\")\n\nprint(\"\\n Stability:\")\nfor method, instability_list in all_instability_values.items():\n    instability_array = np.array(instability_list)\n    mean_instability = instability_array.mean()\n    stability = 1 - mean_instability\n    print(f\"- {method}: {stability:.5f}\")\n\nprint(\"\\n Average execution time:\")\nfor method, duration_list in all_duration_values.items():\n    duration_array = np.array(duration_list)\n    mean_duration = duration_array.mean()\n    print(f\"- {method}: {mean_duration:.5f}\")\n\nprint(\"\\n Valley Score (Mean ± Std):\")\nfor method, ms_list in all_valley_scores.items():\n    ms_array = np.array(ms_list)\n    mean_ms = ms_array.mean()\n    std_ms = ms_array.std()\n    print(f\"- {method}: {mean_ms:.2f} ± {std_ms:.2f}\")\n\nprint(\"\\n Sufficiency Score (Mean ± Std):\")\nfor method, score in all_stability_scores.items():\n    score_array = np.array(score)\n    mean_score = score_array.mean()\n    std_score = score_array.std()\n    print(f\"- {method}: {mean_score:.2f} ± {std_score:.2f}\")\n\n\nsummary_data = []\n\nfor method in all_ec_values.keys():\n    ec_array = np.array(all_ec_values[method])\n    rqi_array = np.array(all_rqi_values.get(method, [np.nan] * len(ec_array)))\n    instability_array = np.array(all_instability_values.get(method, [np.nan] * len(ec_array)))\n    duration_array = np.array(all_duration_values.get(method, [np.nan] * len(ec_array)))\n    ms_array = np.array(all_valley_scores[method])\n    score_array = np.array(all_stability_scores[method])\n    \n    summary_data.append({\n        \"Method\": method,\n        \"EC_Mean\": ec_array.mean(),\n        \"EC_Std\": ec_array.std(),\n        \"RQI_Mean\": rqi_array.mean(),\n        \"RQI_Std\": rqi_array.std(),\n        \"Stability\": 1 - instability_array.mean(),\n        \"Time\": duration_array.mean(),\n        \"Valley Mean\": ms_array.mean(),\n        \"Valley Std\": ms_array.std(),\n        \"Suff Mean\": score_array.mean(),\n        \"Suff Std\": score_array.std()\n    })\n\n# --- Create DataFrame ---\ndf_summary = pd.DataFrame(summary_data)\n\n# --- Save to CSV ---\ndf_summary.to_csv(\"explanation_metrics_summary.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the CSV file\ndf = pd.read_csv(\"explanation_metrics_summary.csv\")\n\n# Show the contents\nprint(df)\n\nend_notebook_time = datetime.now()\n\nduration = end_notebook_time - start_notebook_time\ntotal_seconds = int(duration.total_seconds())\n\nhours = total_seconds // 3600\nminutes = (total_seconds % 3600) // 60\nseconds = total_seconds % 60\n\nprint(f\"Notebook time length: {hours} ore, {minutes} minuti, {seconds} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T15:06:32.756191Z","iopub.execute_input":"2025-07-12T15:06:32.756774Z","iopub.status.idle":"2025-07-12T15:06:32.761575Z","shell.execute_reply.started":"2025-07-12T15:06:32.756752Z","shell.execute_reply":"2025-07-12T15:06:32.760920Z"}},"outputs":[],"execution_count":null}]}